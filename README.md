# LLM QA Assistant â€“ Compare & Validate QA Tasks with Local LLMs

Compare outputs from 3 locally hosted LLMs using [Ollama](https://ollama.com).  
This tool helps QA professionals validate AI outputs for test automation, bug triage, and more.

---

## ðŸš€ Features

- Compare 3 open-source LLMs at once
- Fully offline with Ollama
- Fast, lightweight, and private
- Simple, responsive UI with QA-focused tools

---

## ðŸ§± Requirements

- [Ollama](https://ollama.com/)
- [VS Code](https://code.visualstudio.com/) + Live Server

---

## ðŸŽ¯ QA-Specific Tasks Supported

- Generate test cases from requirements
- Convert scenarios to Gherkin
- Summarize test failures or logs
- Generate assertion code from plain language
- Compare coverage across models
- Explain automation errors (stack traces, type errors)

---

## ðŸ“¦ Setup Instructions

```bash
git clone https://github.com/your-username/llm-qa-assistant.git
cd llm-qa-assistant

ollama pull phi3:medium-128k
ollama pull deepseek-r1:8b
ollama pull qwen:1.8b

ollama serve
```

Open `index.html` using Live Server in VS Code.  
Type a QA prompt and click "Compare Models".

---

## ðŸ“· Screenshot

Example: Comparing test case generation for a login form.

![QA LLM Comparator UI](image.png)

---

## ðŸ“ƒ License

MIT â€” free for personal or commercial use.

---

## ðŸ™Œ Contribute

Pull requests are welcome!  
Open issues to suggest improvements or request more QA prompt templates.
